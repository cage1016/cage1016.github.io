<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>GCS - Tag - KaiChu</title><link>https://kaichu.io/tags/gcs/</link><description>GCS - Tag - KaiChu</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Tue, 28 Jul 2015 12:03:48 +0800</lastBuildDate><atom:link href="https://kaichu.io/tags/gcs/" rel="self" type="application/rss+xml"/><item><title>GCSIterator (Python CSV iterator for Google Cloud Storage) via GAE</title><link>https://kaichu.io/posts/gcsiterator/</link><pubDate>Tue, 28 Jul 2015 12:03:48 +0800</pubDate><author>Author</author><guid>https://kaichu.io/posts/gcsiterator/</guid><description>最近的專案常常需要在 GAE - Python 跟大 CSV (40MB)檔打交道。在 Python 中利用 `csv.reader` &amp; `csv.DictReader` 可以很容易的處理 `csv` 讀取的動作。但是在 GAE 平台上一般 Request 時間只有 **60s**，而 Tasks Request 則有 **10mins** 的限制[3]，而在 GAE 上處理超大檔案的時候除了會遇到 `DeadlineExceededErrors` 的雷也會踩到 `Exceeded soft private memory limit` 的問題(預設 instance 的記憶體只有 **128MB**，在處理大 CSV 檔很容易踩到的雷)</description></item></channel></rss>