<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>csv on KaiChu</title><link>https://kaichu.io/tags/csv/</link><description>Recent content in csv on KaiChu</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Tue, 28 Jul 2015 12:03:48 +0800</lastBuildDate><atom:link href="https://kaichu.io/tags/csv/index.xml" rel="self" type="application/rss+xml"/><item><title>GCSIterator (Python CSV iterator for Google Cloud Storage) via GAE</title><link>https://kaichu.io/posts/gcsiterator/</link><pubDate>Tue, 28 Jul 2015 12:03:48 +0800</pubDate><guid>https://kaichu.io/posts/gcsiterator/</guid><description>&lt;p>最近的專案常常需要在 &lt;a href="https://cloud.google.com/appengine/docs/python/">GAE - Python&lt;/a> 跟大 CSV (40MB)檔打交道。在 Python 中利用 &lt;code>csv.reader&lt;/code> &amp;amp; &lt;code>csv.DictReader&lt;/code>
可以很容易的處理 &lt;code>csv&lt;/code> 讀取的動作。但是在 GAE 平台上一般 Request 時間只有 &lt;strong>60s&lt;/strong>，而 Tasks Request 則有 &lt;strong>10mins&lt;/strong> 的限制[3]，而在 GAE 上處理超大檔案的時候除了會遇到
&lt;code>DeadlineExceededErrors&lt;/code> 的雷也會踩到 &lt;code>Exceeded soft private memory limit&lt;/code> 的問題(預設 instance 的記憶體只有 &lt;strong>128MB&lt;/strong>，在處理大 CSV 檔很容易踩到的雷)&lt;/p>
&lt;p>所以在處理大 CSV 檔最好不要一次就把所有的資料讀到記憶體中，而 GAE 上又有檔案存取的限制，所以大部份會搭配 &lt;a href="https://cloud.google.com/storage/">GCS&lt;/a> 一起使用，
把檔案放在 GCS 上，由 GAE 透過 &lt;code>google-api-python-client&lt;/code> 到 GCS 進行檔案的存取&lt;/p>
&lt;p>&lt;code>google-api-python-client&lt;/code> 中實作了 GCS JSON API 的 chunks 下載(&lt;strong>MediaIoBaseDownload&lt;/strong> [4])，在 chunks 下載時就必需另外處理斷行的問題(實作 Python csv.DictReader iterator 內解決斷行問題)&lt;/p></description></item></channel></rss>